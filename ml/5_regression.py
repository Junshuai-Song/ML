# coding=utf-8
'''
Created on 2016年6月1日
@author: a1

第五课：回归

都用极大似然估计来做，前提假设：变量服从某种分布。
极大似然估计计算，需要求极值，此时使用梯度下降法。

线性回归：连续数据预测
（假设高斯分布）
要解决的问题：求出系数参数。
1. 是什么？
    简单说就是一个需要拟合的方程式，其中系数未知。
2. 怎么解系数参数？
    对于残差，我们从中心极限定理的角度，认为其满足高斯分布。
    替换
    得到y = f(x,系数变量)
    
    之后得到关于系数变量的极大似然函数。
    化简到最后，其实就是最小二乘的方差和最小的目标函数！！！
    
    之后求解这个最小二乘的参数估计值：求最小值（已经是导数）
    化简后得到一个关于参数值的目标函数，化成矩阵的形式
    之后求偏导得驻点，可以获得结果。
    
    注意，这里当矩阵XTX不可逆或者防止过拟合，需要添加扰动项
    （一般都可逆）
    当某个估计参数为0时，其实表示当前特征没用，也起到一个降维的作用。
    
    这个扰动项，其实就是调参的一个过程。其属于超参数，没办法从目标函数求出，只能调节！！！
    （这个参数>0，但是上界不确定；一般会使得拟合曲线更加光滑）
3. 机器学习与数据使用：如何评价模型好坏？
    训练数据、验证数据、测试数据
    交叉验证：十折交叉验证

4. 另一种计算参数的方法：梯度下降法
    初始化参数：随机
    有了极大似然获得的目标函数，沿着负梯度方向迭代，更新后的参数，使得目标函数更小（求最小值的目标函数）
    训练参数：
        a：学习率、步长（这个是有办法找到比较优的，下次说；目前先设定为0.01）
    
    对于线性回归来说！目标函数相对于某参数变量！是凸函数！所以都能保证是最优解！
    
    SGD：随机梯度下降（首选，但可能会不很精确、到局部最小值，但更快）
    BGD：批量梯度下降
    mini-batch：折中，若干个样本一起做一个梯度，即不是一个、也不是全部。- 其实没什么人用

5. 线性回归进一步分析：
    可以对样本是非线性的，只要对参数是线性的即可。
    拟合结果为曲线，但是也叫线性回归、也属于线性模型！！线性指对参数theta的线性！
    例如：
        y = ax + bx^2 
6. 局部加权回归：
    多加一个权值w：高斯核函数（在SVM章节详细讨论）
    - 表示与x的距离，远近权重不同
    - 遗留问题：核函数到底代表什么意义？为什么会提出它？
7. 时间序列与线性回归不同：
    时间序列x的顺序是不能打乱的！ 线性回归没关系。
8. 数据有顺序时做预测：
    隐马尔科夫模型 ...等可以、或者自回归
    
用回归解决分类问题：
    并不可取。目标不一样！我们这里并不是希望残差和最小。
    那是希望什么？
    在某个参数的情况下，生成现在分布的可能最大。
    
Logistic回归：离散数据
    （与线性回归的根本区别：logistic回归假设两点分布：伯努利分布，不是高斯分布！！）
    长的样子：sigmoid函数
1. 参数估计：
    假定：
    P(y=1) = h(x)
    p(y=0) = 1 - h(x)
    求出一个似然函数->取对数->对参数求偏导->沿着梯度上升（这里是要求一个最大似然函数值）
    （参数theta的函数是一个凹函数，可以保证最优）
    
    其实是一个广义的线性模型。logit线性回归模型。
    
    特征可以组合：(1,x1,x2,x1^2...)，但是解释性较差。
    特征组合之后，分类面不再是线性的！！！
    
2. Softmax回归：三分类问题
    前提假设类似



'''

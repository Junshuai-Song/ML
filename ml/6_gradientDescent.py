# coding=utf-8
'''
Created on 2016年6月3日
@author: a1

梯度下降 & 拟牛顿
1. 普通梯度下降存在问题
    目标函数是凸的，不断下降，似乎结果一定是最优。
    但是存在：
    （1）a学习率、步长的确定    --> 学习率设置多大？固定还是变化？
    （2）下降方向：处理梯度方向，其他方向是否可以？ --> 可行方向和梯度方向有什么关系？

2. 求解y = x^2的最小值
    初始值x=1.5，学习率使用0.01
    x = x - a*f'(x)    沿着x处导数的反方向下降
    
3. 已经论证
    最小二乘建立的目标函数，是在高斯噪声的假设下，利用极大似然估计建立的。、
    
4. 实践中，发现收敛很慢，我们就需要去做一些事情。
    即：调整学习率
    迭代开始初期，可以使用较大学习率；迭代后期，使用较小的学习率增加稳定性和精度。
    如何构造学习率a？

5. 回溯线性搜索和二次插值线性搜索能够基本满足实践中的需要
    推广：拟合三次曲线进行插值。
6. 牛顿迭代公式：
    利用泰勒展开式，去掉高阶项的一个近似值！
    含义：实际上是利用一个已知点、当前点一阶导、二阶导！ 之后拟合了一个二次曲线；拿当前二次曲线的最优值梯度方向！
    限制：可能不收敛！计算过程需要计算目标函数的二阶偏导数的逆（时间复杂度较高）
7. 实践当中，我们会选择一个近似牛顿的方向：拟牛顿
    求Hessian矩阵的逆影响算法效率，同时搜索方向并非严格需要负梯度方向或者牛顿方向。
    因此，可以用近似矩阵代替Hessian矩阵，只要满足该矩阵正定、容易求逆
    DFP
    BFGS

个人感觉可以同一数据做多此随机梯度下降；看最后收敛到的值的一个分布，选频率最多的那个。


'''


if __name__ =="__main__":
    print ()
